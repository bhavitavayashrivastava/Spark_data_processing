{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = []\n",
    "\n",
    "for i in range(10000):\n",
    "    lst.append([str(i),\"name\"+str(i)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_rdd = spark.sparkContext.parallelize(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_df = spark.createDataFrame(lst_rdd,[\"id\",\"name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id| name|\n",
      "+---+-----+\n",
      "|  0|name0|\n",
      "|  1|name1|\n",
      "|  2|name2|\n",
      "|  3|name3|\n",
      "|  4|name4|\n",
      "+---+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lst_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_df.coalesce(1).write.mode(\"overwrite\").saveAsTable(\"unbuket_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst_df.coalesce(1).write.bucketBy(4,\"id\").sortBy(\"id\").mode(\"overwrite\").saveAsTable(\"buket_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|  id|    name|\n",
      "+----+--------+\n",
      "|1000|name1000|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_read1 = spark.table(\"unbuket_demo\")\n",
    "df_read1.where(\"id='1000'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------+\n",
      "|  id|    name|\n",
      "+----+--------+\n",
      "|1000|name1000|\n",
      "+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_read = spark.table(\"buket_demo\")\n",
    "df_read.where(\"id='1000'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [id#26, name#27]\n",
      "+- *(1) Filter (isnotnull(id#26) && (id#26 = 1000))\n",
      "   +- *(1) FileScan parquet default.unbuket_demo[id#26,name#27] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/admin/Documents/spark_dataAnalytics_machine_learning/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id), EqualTo(id,1000)], ReadSchema: struct<id:string,name:string>\n"
     ]
    }
   ],
   "source": [
    "df_read1.where(\"id='1000'\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [id#37, name#38]\n",
      "+- *(1) Filter (isnotnull(id#37) && (id#37 = 1000))\n",
      "   +- *(1) FileScan parquet default.buket_demo[id#37,name#38] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/C:/Users/admin/Documents/spark_dataAnalytics_machine_learning/spark-wareh..., PartitionFilters: [], PushedFilters: [IsNotNull(id), EqualTo(id,1000)], ReadSchema: struct<id:string,name:string>, SelectedBucketsCount: 1 out of 4\n"
     ]
    }
   ],
   "source": [
    "df_read.where(\"id='1000'\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                  id|              string|   null|\n",
      "|                name|              string|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             default|       |\n",
      "|               Table|        unbuket_demo|       |\n",
      "|        Created Time|Mon Dec 07 12:29:...|       |\n",
      "|         Last Access|Thu Jan 01 05:29:...|       |\n",
      "|          Created By|         Spark 2.4.4|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|            Location|file:/C:/Users/ad...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe extended unbuket_demo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------+\n",
      "|            col_name|           data_type|comment|\n",
      "+--------------------+--------------------+-------+\n",
      "|                  id|              string|   null|\n",
      "|                name|              string|   null|\n",
      "|                    |                    |       |\n",
      "|# Detailed Table ...|                    |       |\n",
      "|            Database|             default|       |\n",
      "|               Table|          buket_demo|       |\n",
      "|        Created Time|Mon Dec 07 12:33:...|       |\n",
      "|         Last Access|Thu Jan 01 05:29:...|       |\n",
      "|          Created By|         Spark 2.4.4|       |\n",
      "|                Type|             MANAGED|       |\n",
      "|            Provider|             parquet|       |\n",
      "|         Num Buckets|                   4|       |\n",
      "|      Bucket Columns|              [`id`]|       |\n",
      "|        Sort Columns|              [`id`]|       |\n",
      "|            Location|file:/C:/Users/ad...|       |\n",
      "+--------------------+--------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"describe extended buket_demo\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
