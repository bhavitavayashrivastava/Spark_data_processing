{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "#a = Row(name='Antony',age=28,height=180)\n",
    "\n",
    "#print('a: ',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (SparkSession.builder.appName('Rdd_change').getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://bhavitavaya:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.4</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Rdd_change</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x816d550>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on SparkSession in module pyspark.sql.session object:\n",
      "\n",
      "class SparkSession(builtins.object)\n",
      " |  SparkSession(sparkContext, jsparkSession=None)\n",
      " |  \n",
      " |  The entry point to programming Spark with the Dataset and DataFrame API.\n",
      " |  \n",
      " |  A SparkSession can be used create :class:`DataFrame`, register :class:`DataFrame` as\n",
      " |  tables, execute SQL over tables, cache tables, and read parquet files.\n",
      " |  To create a SparkSession, use the following builder pattern:\n",
      " |  \n",
      " |  >>> spark = SparkSession.builder \\\n",
      " |  ...     .master(\"local\") \\\n",
      " |  ...     .appName(\"Word Count\") \\\n",
      " |  ...     .config(\"spark.some.config.option\", \"some-value\") \\\n",
      " |  ...     .getOrCreate()\n",
      " |  \n",
      " |  .. autoattribute:: builder\n",
      " |     :annotation:\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |      Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  __exit__(self, exc_type, exc_val, exc_tb)\n",
      " |      Enable 'with SparkSession.builder.(...).getOrCreate() as session: app' syntax.\n",
      " |      \n",
      " |      Specifically stop the SparkSession on exit of the with block.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  __init__(self, sparkContext, jsparkSession=None)\n",
      " |      Creates a new SparkSession.\n",
      " |      \n",
      " |      >>> from datetime import datetime\n",
      " |      >>> spark = SparkSession(sc)\n",
      " |      >>> allTypes = sc.parallelize([Row(i=1, s=\"string\", d=1.0, l=1,\n",
      " |      ...     b=True, list=[1, 2, 3], dict={\"s\": 0}, row=Row(a=1),\n",
      " |      ...     time=datetime(2014, 8, 1, 14, 1, 5))])\n",
      " |      >>> df = allTypes.toDF()\n",
      " |      >>> df.createOrReplaceTempView(\"allTypes\")\n",
      " |      >>> spark.sql('select i+1, d+1, not b, list[1], dict[\"s\"], time, row.a '\n",
      " |      ...            'from allTypes where b and i > 0').collect()\n",
      " |      [Row((i + CAST(1 AS BIGINT))=2, (d + CAST(1 AS DOUBLE))=2.0, (NOT b)=False, list[1]=2,             dict[s]=0, time=datetime.datetime(2014, 8, 1, 14, 1, 5), a=1)]\n",
      " |      >>> df.rdd.map(lambda x: (x.i, x.s, x.d, x.l, x.b, x.time, x.row.a, x.list)).collect()\n",
      " |      [(1, 'string', 1.0, 1, True, datetime.datetime(2014, 8, 1, 14, 1, 5), 1, [1, 2, 3])]\n",
      " |  \n",
      " |  createDataFrame(self, data, schema=None, samplingRatio=None, verifySchema=True)\n",
      " |      Creates a :class:`DataFrame` from an :class:`RDD`, a list or a :class:`pandas.DataFrame`.\n",
      " |      \n",
      " |      When ``schema`` is a list of column names, the type of each column\n",
      " |      will be inferred from ``data``.\n",
      " |      \n",
      " |      When ``schema`` is ``None``, it will try to infer the schema (column names and types)\n",
      " |      from ``data``, which should be an RDD of :class:`Row`,\n",
      " |      or :class:`namedtuple`, or :class:`dict`.\n",
      " |      \n",
      " |      When ``schema`` is :class:`pyspark.sql.types.DataType` or a datatype string, it must match\n",
      " |      the real data, or an exception will be thrown at runtime. If the given schema is not\n",
      " |      :class:`pyspark.sql.types.StructType`, it will be wrapped into a\n",
      " |      :class:`pyspark.sql.types.StructType` as its only field, and the field name will be \"value\",\n",
      " |      each record will also be wrapped into a tuple, which can be converted to row later.\n",
      " |      \n",
      " |      If schema inference is needed, ``samplingRatio`` is used to determined the ratio of\n",
      " |      rows used for schema inference. The first row will be used if ``samplingRatio`` is ``None``.\n",
      " |      \n",
      " |      :param data: an RDD of any kind of SQL data representation(e.g. row, tuple, int, boolean,\n",
      " |          etc.), or :class:`list`, or :class:`pandas.DataFrame`.\n",
      " |      :param schema: a :class:`pyspark.sql.types.DataType` or a datatype string or a list of\n",
      " |          column names, default is ``None``.  The data type string format equals to\n",
      " |          :class:`pyspark.sql.types.DataType.simpleString`, except that top level struct type can\n",
      " |          omit the ``struct<>`` and atomic types use ``typeName()`` as their format, e.g. use\n",
      " |          ``byte`` instead of ``tinyint`` for :class:`pyspark.sql.types.ByteType`. We can also use\n",
      " |          ``int`` as a short name for ``IntegerType``.\n",
      " |      :param samplingRatio: the sample ratio of rows used for inferring\n",
      " |      :param verifySchema: verify data types of every row against schema.\n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      .. versionchanged:: 2.1\n",
      " |         Added verifySchema.\n",
      " |      \n",
      " |      .. note:: Usage with spark.sql.execution.arrow.enabled=True is experimental.\n",
      " |      \n",
      " |      >>> l = [('Alice', 1)]\n",
      " |      >>> spark.createDataFrame(l).collect()\n",
      " |      [Row(_1='Alice', _2=1)]\n",
      " |      >>> spark.createDataFrame(l, ['name', 'age']).collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> d = [{'name': 'Alice', 'age': 1}]\n",
      " |      >>> spark.createDataFrame(d).collect()\n",
      " |      [Row(age=1, name='Alice')]\n",
      " |      \n",
      " |      >>> rdd = sc.parallelize(l)\n",
      " |      >>> spark.createDataFrame(rdd).collect()\n",
      " |      [Row(_1='Alice', _2=1)]\n",
      " |      >>> df = spark.createDataFrame(rdd, ['name', 'age'])\n",
      " |      >>> df.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> from pyspark.sql import Row\n",
      " |      >>> Person = Row('name', 'age')\n",
      " |      >>> person = rdd.map(lambda r: Person(*r))\n",
      " |      >>> df2 = spark.createDataFrame(person)\n",
      " |      >>> df2.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> from pyspark.sql.types import *\n",
      " |      >>> schema = StructType([\n",
      " |      ...    StructField(\"name\", StringType(), True),\n",
      " |      ...    StructField(\"age\", IntegerType(), True)])\n",
      " |      >>> df3 = spark.createDataFrame(rdd, schema)\n",
      " |      >>> df3.collect()\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      \n",
      " |      >>> spark.createDataFrame(df.toPandas()).collect()  # doctest: +SKIP\n",
      " |      [Row(name='Alice', age=1)]\n",
      " |      >>> spark.createDataFrame(pandas.DataFrame([[1, 2]])).collect()  # doctest: +SKIP\n",
      " |      [Row(0=1, 1=2)]\n",
      " |      \n",
      " |      >>> spark.createDataFrame(rdd, \"a: string, b: int\").collect()\n",
      " |      [Row(a='Alice', b=1)]\n",
      " |      >>> rdd = rdd.map(lambda row: row[1])\n",
      " |      >>> spark.createDataFrame(rdd, \"int\").collect()\n",
      " |      [Row(value=1)]\n",
      " |      >>> spark.createDataFrame(rdd, \"boolean\").collect() # doctest: +IGNORE_EXCEPTION_DETAIL\n",
      " |      Traceback (most recent call last):\n",
      " |          ...\n",
      " |      Py4JJavaError: ...\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  newSession(self)\n",
      " |      Returns a new SparkSession as new session, that has separate SQLConf,\n",
      " |      registered temporary views and UDFs, but shared SparkContext and\n",
      " |      table cache.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  range(self, start, end=None, step=1, numPartitions=None)\n",
      " |      Create a :class:`DataFrame` with single :class:`pyspark.sql.types.LongType` column named\n",
      " |      ``id``, containing elements in a range from ``start`` to ``end`` (exclusive) with\n",
      " |      step value ``step``.\n",
      " |      \n",
      " |      :param start: the start value\n",
      " |      :param end: the end value (exclusive)\n",
      " |      :param step: the incremental step (default: 1)\n",
      " |      :param numPartitions: the number of partitions of the DataFrame\n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> spark.range(1, 7, 2).collect()\n",
      " |      [Row(id=1), Row(id=3), Row(id=5)]\n",
      " |      \n",
      " |      If only one argument is specified, it will be used as the end value.\n",
      " |      \n",
      " |      >>> spark.range(3).collect()\n",
      " |      [Row(id=0), Row(id=1), Row(id=2)]\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  sql(self, sqlQuery)\n",
      " |      Returns a :class:`DataFrame` representing the result of the given query.\n",
      " |      \n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> df.createOrReplaceTempView(\"table1\")\n",
      " |      >>> df2 = spark.sql(\"SELECT field1 AS f1, field2 as f2 from table1\")\n",
      " |      >>> df2.collect()\n",
      " |      [Row(f1=1, f2='row1'), Row(f1=2, f2='row2'), Row(f1=3, f2='row3')]\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  stop(self)\n",
      " |      Stop the underlying :class:`SparkContext`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  table(self, tableName)\n",
      " |      Returns the specified table as a :class:`DataFrame`.\n",
      " |      \n",
      " |      :return: :class:`DataFrame`\n",
      " |      \n",
      " |      >>> df.createOrReplaceTempView(\"table1\")\n",
      " |      >>> df2 = spark.table(\"table1\")\n",
      " |      >>> sorted(df.collect()) == sorted(df2.collect())\n",
      " |      True\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  catalog\n",
      " |      Interface through which the user may create, drop, alter or query underlying\n",
      " |      databases, tables, functions etc.\n",
      " |      \n",
      " |      :return: :class:`Catalog`\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  conf\n",
      " |      Runtime configuration interface for Spark.\n",
      " |      \n",
      " |      This is the interface through which the user can get and set all Spark and Hadoop\n",
      " |      configurations that are relevant to Spark SQL. When getting the value of a config,\n",
      " |      this defaults to the value set in the underlying :class:`SparkContext`, if any.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  read\n",
      " |      Returns a :class:`DataFrameReader` that can be used to read data\n",
      " |      in as a :class:`DataFrame`.\n",
      " |      \n",
      " |      :return: :class:`DataFrameReader`\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  readStream\n",
      " |      Returns a :class:`DataStreamReader` that can be used to read data streams\n",
      " |      as a streaming :class:`DataFrame`.\n",
      " |      \n",
      " |      .. note:: Evolving.\n",
      " |      \n",
      " |      :return: :class:`DataStreamReader`\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  sparkContext\n",
      " |      Returns the underlying :class:`SparkContext`.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  streams\n",
      " |      Returns a :class:`StreamingQueryManager` that allows managing all the\n",
      " |      :class:`StreamingQuery` StreamingQueries active on `this` context.\n",
      " |      \n",
      " |      .. note:: Evolving.\n",
      " |      \n",
      " |      :return: :class:`StreamingQueryManager`\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  udf\n",
      " |      Returns a :class:`UDFRegistration` for UDF registration.\n",
      " |      \n",
      " |      :return: :class:`UDFRegistration`\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  version\n",
      " |      The version of Spark on which this application is running.\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  Builder = <class 'pyspark.sql.session.SparkSession.Builder'>\n",
      " |      Builder for :class:`SparkSession`.\n",
      " |  \n",
      " |  builder = <pyspark.sql.session.SparkSession.Builder object>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SparkSession' object has no attribute 'textFile'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-bfdf7bdfa217>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhelp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'SparkSession' object has no attribute 'textFile'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
